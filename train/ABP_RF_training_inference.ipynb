{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aca8ac7-75e0-4c9e-a1c2-7c1bd7b45825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/9401_qinzhiwei/.conda/envs/esm-nlp/lib/python3.8/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/nfs/home/9401_qinzhiwei/.conda/envs/esm-nlp/lib/python3.8/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/nfs/home/9401_qinzhiwei/.conda/envs/esm-nlp/lib/python3.8/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/nfs/home/9401_qinzhiwei/.conda/envs/esm-nlp/lib/python3.8/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import EsmTokenizer, EsmForMaskedLM\n",
    "from peft import PeftModel\n",
    "from umap import UMAP\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import shap\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from joblib.parallel import BatchCompletionCallBack\n",
    "\n",
    "############################################\n",
    "# Step 1: Read the input CSV file\n",
    "############################################\n",
    "df = pd.read_csv('./ABP_RF_SHAP_data/A_domain_786.csv')\n",
    "\n",
    "#########################################################################\n",
    "# 1. Data Augmentation: Add duplicate samples for substrates with < 10 occurrences\n",
    "#########################################################################\n",
    "\n",
    "# Count the occurrences of each substrate\n",
    "substrate_counts = df['substrate'].value_counts()\n",
    "\n",
    "# Identify substrates with occurrences less than 10\n",
    "low_occurrence = substrate_counts[substrate_counts < 10]\n",
    "\n",
    "# Create a copy of the original DataFrame for augmentation\n",
    "df_enhanced = df.copy()\n",
    "new_rows = []\n",
    "\n",
    "# Add duplicate samples for low-frequency substrates\n",
    "for substrate, count in low_occurrence.items():\n",
    "    substrate_samples = df[df['substrate'] == substrate]\n",
    "    copies_needed = 10 - count\n",
    "\n",
    "    for i in range(copies_needed):\n",
    "        # Randomly select a sample to duplicate\n",
    "        sample_to_copy = substrate_samples.sample(1, random_state=np.random.randint(10000)).iloc[0]\n",
    "        new_row = sample_to_copy.copy()\n",
    "        new_row['id'] = f\"{new_row['id']}_repeat_{i+1}\"  # Modify ID to indicate duplication\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# Add the new rows to the augmented dataset\n",
    "if new_rows:\n",
    "    df_enhanced = pd.concat([df_enhanced, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "#########################################################################\n",
    "# 2. Encode substrates as numeric values\n",
    "#########################################################################\n",
    "\n",
    "# Factorize the substrates in the augmented dataset\n",
    "codes, uniques = pd.factorize(df_enhanced['substrate'])\n",
    "df_enhanced['substrate_numeric'] = codes\n",
    "\n",
    "# Create a mapping dictionary and apply it to the original dataset\n",
    "mapping = dict(zip(uniques, range(len(uniques))))\n",
    "df['substrate_numeric'] = df['substrate'].map(mapping)\n",
    "\n",
    "#########################################################################\n",
    "# 3. Save the augmented CSV and FASTA files\n",
    "#########################################################################\n",
    "\n",
    "# Save the augmented dataset as a CSV file\n",
    "output_csv = './ABP_RF_SHAP_data/A_domain_augmented_1579.csv'\n",
    "df_enhanced.to_csv(output_csv, index=False)\n",
    "\n",
    "# Generate the augmented dataset as a FASTA file\n",
    "records = [\n",
    "    SeqRecord(Seq(row['ABP']), id=str(row['id']), description=\"\")\n",
    "    for _, row in df_enhanced.iterrows()\n",
    "]\n",
    "output_fasta = './ABP_RF_SHAP_data/A_domain_augmented_1579.fasta'\n",
    "with open(output_fasta, 'w') as output_handle:\n",
    "    SeqIO.write(records, output_handle, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d217907-a59c-4326-af36-1520a8e0dba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing LoRA embeddings for all sequences:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing embeddings: 100%|██████████| 1579/1579 [00:43<00:00, 36.05it/s]\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "# 4. Load the ABP-ESM2 for ABP embedding\n",
    "#########################################################################\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = '/nfs/home/9401_qinzhiwei/HJQ/1.NRPS/finetune/esm/esm2_t33_650M_UR50D'\n",
    "tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "base_model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Load the LoRA fine-tuned model\n",
    "lora_model = PeftModel.from_pretrained(base_model, '/nfs/home/9401_qinzhiwei/HJQ/1.NRPS/3.CPSL/pocket/PLM')\n",
    "lora_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Configure device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lora_model.to(device)\n",
    "\n",
    "def compute_lora_embedding(sequence):\n",
    "    \"\"\"\n",
    "    Compute LoRA embedding for a single protein sequence.\n",
    "    \n",
    "    Args:\n",
    "        sequence (str): Protein sequence as a string.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Embedding tensor with shape [seq_length, hidden_dim],\n",
    "                      where seq_length = len(sequence).\n",
    "    \"\"\"\n",
    "    # Tokenize the input sequence\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Perform forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        output = lora_model.esm(**inputs).last_hidden_state[0]\n",
    "    \n",
    "    # Exclude CLS token and take only the actual sequence embeddings\n",
    "    embedding = output[1:len(sequence)+1].cpu()\n",
    "    return embedding\n",
    "\n",
    "# Compute LoRA embeddings for all sequences in the augmented dataset\n",
    "print(\"\\nComputing LoRA embeddings for all sequences:\")\n",
    "lora_embeddings_1579 = []\n",
    "\n",
    "# Iterate over all sequences in the DataFrame\n",
    "for seq in tqdm(df_enhanced['ABP'], desc=\"Computing embeddings\"):\n",
    "    emb = compute_lora_embedding(seq)\n",
    "    lora_embeddings_1579.append(emb)\n",
    "\n",
    "# Convert the list of embeddings to a tensor\n",
    "lora_tensor_1579 = torch.stack(lora_embeddings_1579, dim=0)  # Shape: (N, seq_length, hidden_dim)\n",
    "\n",
    "# # Save the embeddings as a file for reuse\n",
    "# torch.save(lora_tensor_1579, \"lora_embeddings_1579.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae295b31-4b63-43b6-a40b-04e7ba9b8cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LoRA embeddings tensor shape: torch.Size([1579, 27, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Optional: load the saved pt file\n",
    "# File path to the saved tensor\n",
    "embedding_file_1579 = \"lora_embeddings_1579.pt\"\n",
    "\n",
    "# Load the tensor from the file\n",
    "lora_tensor_1579 = torch.load(embedding_file_1579)\n",
    "\n",
    "# Print the shape of the loaded tensor for verification\n",
    "print(\"Loaded LoRA embeddings tensor shape:\", lora_tensor_1579.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41afa24a-84d6-4e12-993d-9c1009f10e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For df_enhanced: Number of sequences: 1579, Sequence length: 27, Hidden dimension: 1280\n",
      "After UMAP dimensionality reduction for df_enhanced:\n",
      "X_umap_1579 shape: (1579, 27)\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# 5. UMAP Dimensionality Reduction: Reduce embeddings at each sequence position to 1D\n",
    "######################################################\n",
    "\n",
    "# Convert the LoRA tensor to a NumPy array\n",
    "X_1579 = lora_tensor_1579.numpy()  # Shape: (1579, seq_length, hidden_dim)\n",
    "\n",
    "# Extract dimensions\n",
    "N_1579, seq_length, hidden_dim = X_1579.shape\n",
    "print(f\"\\nFor df_enhanced: Number of sequences: {N_1579}, Sequence length: {seq_length}, Hidden dimension: {hidden_dim}\")\n",
    "\n",
    "# Initialize an array to store the UMAP-reduced embeddings\n",
    "X_umap_1579 = np.zeros((N_1579, seq_length))  # Shape: (1579, seq_length)\n",
    "\n",
    "# Perform UMAP dimensionality reduction for each sequence position\n",
    "for pos in range(seq_length):\n",
    "    # Initialize UMAP for 1D reduction\n",
    "    umap = UMAP(n_components=1, random_state=42)\n",
    "    \n",
    "    # Extract embeddings at the current position for all sequences\n",
    "    pos_data = X_1579[:, pos, :]  # Shape: (1579, hidden_dim)\n",
    "    \n",
    "    # Apply UMAP and store the reduced 1D embedding\n",
    "    X_umap_1579[:, pos] = umap.fit_transform(pos_data).squeeze()  # Shape: (1579,)\n",
    "\n",
    "print(\"After UMAP dimensionality reduction for df_enhanced:\")\n",
    "print(f\"X_umap_1579 shape: {X_umap_1579.shape}\")  # Shape: (1579, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a28bda37-5570-4820-80d6-d2d79a1d5690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (1579,)\n",
      "\n",
      "Starting GridSearchCV (5-fold cross-validation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GridSearchCV Progress: 360it [00:36,  9.78it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best cross-validation accuracy: 0.93221418525216\n",
      "Best model saved as: best_random_forest_model_umap.pkl\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# 6. Train Random Forest with GridSearchCV (5-fold cross-validation)\n",
    "###################################\n",
    "\n",
    "# Extract numeric labels for the substrate column\n",
    "y = df_enhanced['substrate_numeric'].values\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize Random Forest classifier and GridSearchCV\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "\n",
    "# Custom progress bar for GridSearchCV\n",
    "n_candidates = len(list(ParameterGrid(param_grid)))\n",
    "cv_folds = grid_search.cv  # cv=5\n",
    "total_tasks = n_candidates * cv_folds\n",
    "\n",
    "class TqdmBatchCompletionCallBack(BatchCompletionCallBack):\n",
    "    def __init__(self, pbar, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pbar = pbar\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.pbar.update(n=self.batch_size)\n",
    "        return super().__call__(*args, **kwargs)\n",
    "\n",
    "old_batch_callback = BatchCompletionCallBack\n",
    "def batch_callback_with_tqdm(*args, **kwargs):\n",
    "    return TqdmBatchCompletionCallBack(main_pbar, *args, **kwargs)\n",
    "import joblib\n",
    "joblib.parallel.BatchCompletionCallBack = batch_callback_with_tqdm\n",
    "\n",
    "# Perform GridSearchCV with progress bar\n",
    "print(\"\\nStarting GridSearchCV (5-fold cross-validation)...\")\n",
    "with tqdm(total=total_tasks, desc=\"GridSearchCV Progress\") as main_pbar:\n",
    "    grid_search.fit(X_umap_1579, y)\n",
    "\n",
    "# Restore original BatchCompletionCallBack\n",
    "joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "\n",
    "# Output best parameters and accuracy\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Save the best model using pickle\n",
    "best_rf = grid_search.best_estimator_\n",
    "model_filename = \"best_random_forest_model_umap.pkl\"\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(best_rf, f)\n",
    "print(f\"Best model saved as: {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe54459a-6ced-461e-bda2-92025b1f58f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model parameters: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 200, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "#Optional: load the ABP-RF trained in this study\n",
    "\n",
    "# Define the filename of the saved model\n",
    "model_filename = \"./ABP_RF_SHAP_data/best_random_forest_model_umap_05012025.pkl\"\n",
    "\n",
    "# Load the model using pickle\n",
    "with open(model_filename, \"rb\") as f:\n",
    "    best_rf = pickle.load(f)\n",
    "\n",
    "# Verify that the model has been loaded successfully by printing its parameters\n",
    "print(\"Loaded best model parameters:\", best_rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7def680-e8e2-4348-955e-71a9aeac221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SHAP values: 100%|██████████| 1579/1579 [00:03<00:00, 424.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all site-level SHAP values (with amino acid) to 'all_samples_site_shap_analysis_with_aa.csv'.\n",
      "Saved site importance statistics to 'site_importance_all_classes.csv'.\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# 8. SHAP Value Analysis\n",
    "###################################\n",
    "\n",
    "# Initialize SHAP TreeExplainer\n",
    "explainer = shap.TreeExplainer(best_rf)\n",
    "shap_values = explainer.shap_values(X_umap_1579)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"SHAP_analysis\", exist_ok=True)\n",
    "\n",
    "# Load the augmented dataset and extract class labels\n",
    "original_data = pd.read_csv('./ABP_RF_SHAP_data/A_domain_augmented_1579.csv')\n",
    "actual_classes = original_data['substrate_numeric'].values\n",
    "\n",
    "# Predict using the trained random forest\n",
    "predictions = best_rf.predict(X_umap_1579)\n",
    "pred_probs = best_rf.predict_proba(X_umap_1579)\n",
    "num_classes = len(shap_values)\n",
    "\n",
    "# SHAP value collection: site-level for all samples and all classes\n",
    "all_shap_rows = []\n",
    "for sample_idx in tqdm(range(len(X_umap_1579)), desc=\"Processing SHAP values\"):\n",
    "    pred_class = predictions[sample_idx]\n",
    "    actual_class = actual_classes[sample_idx]\n",
    "    pocket_sequence = original_data.loc[sample_idx, 'ABP']\n",
    "    for site_idx in range(27):  # Assuming 27 sites (positions)\n",
    "        site_key = f\"Site_{site_idx+1}\"\n",
    "        aa = pocket_sequence[site_idx] if site_idx < len(pocket_sequence) else None\n",
    "        for class_idx in range(num_classes):\n",
    "            shap_val = shap_values[class_idx][sample_idx][site_idx]\n",
    "            all_shap_rows.append({\n",
    "                'Sample_ID': sample_idx,\n",
    "                'Site': site_key,\n",
    "                'Site_Index': site_idx + 1,\n",
    "                'Amino_Acid': aa,\n",
    "                'Predicted_Class': pred_class,\n",
    "                'Actual_Class': actual_class,\n",
    "                'Class': class_idx,\n",
    "                'SHAP_Value': shap_val,\n",
    "                'Abs_SHAP_Value': abs(shap_val)\n",
    "            })\n",
    "\n",
    "# Create DataFrame of all per-site SHAP values, with amino acid info\n",
    "all_shap_df = pd.DataFrame(all_shap_rows)\n",
    "all_shap_df.to_csv('all_samples_site_shap_analysis_with_aa.csv', index=False)\n",
    "print(\"Saved all site-level SHAP values (with amino acid) to 'all_samples_site_shap_analysis_with_aa.csv'.\")\n",
    "\n",
    "# Calculate per-site (across all samples and all classes) importance statistics\n",
    "site_importance = (\n",
    "    all_shap_df\n",
    "    .groupby('Site')['Abs_SHAP_Value']\n",
    "    .agg(['mean', 'std'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'mean': 'Mean_Abs_SHAP', 'std': 'Std_Abs_SHAP'})\n",
    ")\n",
    "site_importance['Importance_Rank'] = site_importance['Mean_Abs_SHAP'].rank(ascending=False)\n",
    "site_importance = site_importance.sort_values('Importance_Rank')\n",
    "\n",
    "# Save summary statistics\n",
    "site_importance.to_csv('site_importance_all_classes.csv', index=False)\n",
    "print(\"Saved site importance statistics to 'site_importance_all_classes.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd208d-20fd-452a-9735-7cf4d322d614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm-nlp (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
